{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Question 1\n",
    "- You can ignore upper/lowercase, punctuation, stopwords... Just count every token you get.\n",
    "- Do not use FreqDist, collections (counter). Just try write a simple count function by yourself. \n",
    "- If you are using dictionary count() function, you may face a running problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import FreqDist\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dict={}\n",
    "# for word in uniwords:\n",
    "#     Dict[word]=words.count(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Question 2\n",
    "- Only need the frequency of 26 charactors, no punctuation. It's NOT words starts with 'a','b'...\n",
    "- You should consider the case problem. Treat 'A' and 'a' as one charactor. \n",
    "- Final result should be ordered by frequency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Question 3\n",
    "- lines means the dialogue for specific actor (actress) not the lines in text content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import webtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ounts to a full pardon. Jack will be free, a privateer in the employ of England.\n",
      "WILL TURNER: Somehow I doubt Jack will consider employment the same as being free.\n",
      "LORD CUTLER BECKETT: Freedom. Jack Sparrow is a dying breed. The world is shrinking, the blank pages of the map filled in. Jack must find his place in the new world or perish. Not unlike you, Mister Turner. You and your fianc√©e face the hangman's noose. \n",
      "WILL TURNER: So you get both Jack *and* the Black Pearl.\n",
      "LORD CUTLER BECKETT: The Black Pearl?\n",
      "WILL TURNER: The property you want that he possesses.\n",
      "LORD CUTLER BECKETT: A ship? Hardly. The item in question's considerably smaller and far more valuable. Something Sparrow keeps on his person at all times. A compass. Ah, you know it. Bring back that Compass, or there's no deal.\n",
      "Scene: BLACK PEARL\n",
      "[Jack is using calipers on a map, using his left hand, a \"P\" brand mark is seen on his right arm]\n",
      "[an hourglass is in the background, Jack taps the Compass]\n",
      "[Jack looks in his bottle, turns the bottle upside-down, only a few drops spill out]\n",
      "JACK SPARROW: Why is the rum always gone?\n",
      "[Jack rises to his feet, staggers]\n",
      "[Jack picks up his hat off the top of a globe]\n",
      "JACK SPARROW: Oh! *That's* why.\n",
      "[Jack walks past the crew sleeping on hammocks, Jack is carrying a lantern]\n",
      "[some of the crew are snoring, and Cotton is among those sleeping]\n",
      "JACK SPARROW: As you were, gents.\n",
      "[Jack goes downstairs to the hold, an animal bleats, Jack unlocks the door, goes in]\n",
      "[Jack sees eerie filter \n"
     ]
    }
   ],
   "source": [
    "print(webtext.raw('pirates.txt')[8000:9500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "- First part, treat all User121 as one person.\n",
    "- Second part, ignore the User information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import nps_chat as nps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['10-19-20s_706posts.xml',\n",
       " '10-19-30s_705posts.xml',\n",
       " '10-19-40s_686posts.xml',\n",
       " '10-19-adults_706posts.xml',\n",
       " '10-24-40s_706posts.xml',\n",
       " '10-26-teens_706posts.xml',\n",
       " '11-06-adults_706posts.xml',\n",
       " '11-08-20s_705posts.xml',\n",
       " '11-08-40s_706posts.xml',\n",
       " '11-08-adults_705posts.xml',\n",
       " '11-08-teens_706posts.xml',\n",
       " '11-09-20s_706posts.xml',\n",
       " '11-09-40s_706posts.xml',\n",
       " '11-09-adults_706posts.xml',\n",
       " '11-09-teens_706posts.xml']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nps.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!-- edited with XMLSpy v2007 sp1 (http://www.altova.com) by Eric Forsyth (Naval Postgraduate School) -->\n",
      "<Session xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:noNamespaceSchemaLocation=\"postClassPOSTagset.xsd\">\n",
      "\t<Posts>\n",
      "\t\t<Post class=\"Statement\" user=\"10-19-20sUser7\">now im left with this gay name<terminals>\n",
      "\t\t\t\t<t pos=\"RB\" word=\"now\"/>\n",
      "\t\t\t\t<t pos=\"PRP\" word=\"im\"/>\n",
      "\t\t\t\t<t pos=\"VBD\" word=\"left\"/>\n",
      "\t\t\t\t<t pos=\"IN\" word=\"with\"/>\n",
      "\t\t\t\t<t pos=\"DT\" word=\"this\"/>\n",
      "\t\t\t\t<t pos=\"JJ\" word=\"gay\"/>\n",
      "\t\t\t\t<t pos=\"NN\" word=\"name\"/>\n",
      "\t\t\t</terminals>\n",
      "\t\t</Post>\n",
      "\t\t<Post class=\"Emotion\" user=\"10-19-20sUse\n"
     ]
    }
   ],
   "source": [
    "print(nps.raw('10-19-20s_706posts.xml')[:600])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "posts = nps.xml_posts()    # Get all posts content from all nps files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5\n",
    "- Print Top 50 words for Shakespeare book and webtext.\n",
    "- compare all words "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6\n",
    "- Should get 34 results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import inaugural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1789-Washington.txt',\n",
       " '1793-Washington.txt',\n",
       " '1797-Adams.txt',\n",
       " '1801-Jefferson.txt',\n",
       " '1805-Jefferson.txt',\n",
       " '1809-Madison.txt',\n",
       " '1813-Madison.txt',\n",
       " '1817-Monroe.txt',\n",
       " '1821-Monroe.txt',\n",
       " '1825-Adams.txt',\n",
       " '1829-Jackson.txt',\n",
       " '1833-Jackson.txt',\n",
       " '1837-VanBuren.txt',\n",
       " '1841-Harrison.txt',\n",
       " '1845-Polk.txt',\n",
       " '1849-Taylor.txt',\n",
       " '1853-Pierce.txt',\n",
       " '1857-Buchanan.txt',\n",
       " '1861-Lincoln.txt',\n",
       " '1865-Lincoln.txt',\n",
       " '1869-Grant.txt',\n",
       " '1873-Grant.txt',\n",
       " '1877-Hayes.txt',\n",
       " '1881-Garfield.txt',\n",
       " '1885-Cleveland.txt',\n",
       " '1889-Harrison.txt',\n",
       " '1893-Cleveland.txt',\n",
       " '1897-McKinley.txt',\n",
       " '1901-McKinley.txt',\n",
       " '1905-Roosevelt.txt',\n",
       " '1909-Taft.txt',\n",
       " '1913-Wilson.txt',\n",
       " '1917-Wilson.txt',\n",
       " '1921-Harding.txt',\n",
       " '1925-Coolidge.txt',\n",
       " '1929-Hoover.txt',\n",
       " '1933-Roosevelt.txt',\n",
       " '1937-Roosevelt.txt',\n",
       " '1941-Roosevelt.txt',\n",
       " '1945-Roosevelt.txt',\n",
       " '1949-Truman.txt',\n",
       " '1953-Eisenhower.txt',\n",
       " '1957-Eisenhower.txt',\n",
       " '1961-Kennedy.txt',\n",
       " '1965-Johnson.txt',\n",
       " '1969-Nixon.txt',\n",
       " '1973-Nixon.txt',\n",
       " '1977-Carter.txt',\n",
       " '1981-Reagan.txt',\n",
       " '1985-Reagan.txt',\n",
       " '1989-Bush.txt',\n",
       " '1993-Clinton.txt',\n",
       " '1997-Clinton.txt',\n",
       " '2001-Bush.txt',\n",
       " '2005-Bush.txt',\n",
       " '2009-Obama.txt']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inaugural.fileids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 8\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import nps_chat, words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abandoner',\n",
       " 'abandonment',\n",
       " 'Abanic',\n",
       " 'Abantes',\n",
       " 'abaptiston',\n",
       " 'Abarambo',\n",
       " 'Abaris',\n",
       " 'abarthrosis',\n",
       " 'abarticular',\n",
       " 'abarticulation',\n",
       " 'abas',\n",
       " 'abase',\n",
       " 'abased',\n",
       " 'abasedly',\n",
       " 'abasedness',\n",
       " 'abasement',\n",
       " 'abaser',\n",
       " 'Abasgi',\n",
       " 'abash',\n",
       " 'abashed']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.words()[50:70]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_posts = list(nps_chat.words('10-19-20s_706posts.xml'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['go', 'U7', 'do', \"n't\", 'golf', 'clap', 'me', '.', 'fuck', 'you', 'U121', ':@', 'whats', 'everyone', 'up', 'to', '?', 'PART', 'PART', 'i', \"'ll\", 'thunder', 'clap', 'your', 'ass', '.', 'PART', 'and', 'i', 'dont', 'even', 'know', 'what', 'that', 'means', '.', 'that', 'sounds', 'painful', 'any', 'ladis', 'wanna', 'chat', '?', '29', 'm', '26', '/', 'm', 'JOIN', 'my', 'cousin', 'drew', 'a', 'messed', 'up', 'pic', 'on', 'my', 'cast', 'PART', '24', '/', 'm', 'boo', '.', '26', '/', 'm', 'and', 'sexy', 'lol', 'U115', 'boo', '.', 'JOIN', 'PART', 'he', 'drew', 'a', 'girl', 'with', 'legs', 'spread', 'boo', '.', 'hope', 'he', 'didnt', 'draw', 'a', 'penis', 'PART', 'ewwwww', 'lol', '&', 'a', 'head', 'between', 'her', 'legs', 'JOIN', 'JOIN', 'sounds', 'good', 'to', 'me', '.', 'r', 'u', 'serious', 'JOIN', 'PART', 'I', \"'ll\", 'take', 'one', ',', 'please', '.', '&', 'i', 'have', 'to', 'go', 'to', 'the', 'docs', 'tomorrow', 'ya', 'man', 'I', 'am', 'too', '..', 'Connected', 'to', '...', 'Slip', 'away', '...', 'Fade', 'away', '...', 'Days', 'away', 'I', '...', 'Still', 'feel', 'you', '...', 'Touching', 'me', '...', 'Changing', 'me', '...', 'Considerably', 'killing', 'me', '...', 'heeeey', '!', 'do', \"n't\", 'you', 'have', 'a', 'sharpie', '?', '26', '/', 'm', 'you', \"'re\", 'back', 'U115', 'U129', 'yep', 'U115', 'Any', 'ladies', 'wanna', 'chat', 'with', '24', '/', 'm', 'hurry', 'ladies', 'PART', 'JOIN', 'JOIN', 'not', 'fast', 'enough', 'U116', 'a', 'bowl', 'i', 'got', 'a', 'blunt', 'an', 'a', 'bong', '......', 'lol', 'JOIN', 'well', ',', 'glad', 'it', 'worked', 'out', 'my', 'chair', 'is', 'too', 'hard', '.', 'Anyone', 'from', 'Tennessee', 'in', 'here', '?', 'hey', 'ladies', 'as', 'am', 'i', 'is', 'U68', 'back', 'yet', 'PART', 'hey', 'PART', 'JOIN', 'U121', 'is', 'missing', 'a', 'B', 'in', 'her', 'name', 'and', 'i', 'do', \"n't\", 'complain', 'about', 'things', 'being', 'hard', 'very', 'often', '.', 'ok', 'yes', 'U30', 'fire', 'it', 'up', 'Any', 'women', 'from', 'Nashville', 'in', 'here', '?', 'JOIN', 'PART', 'and', 'an', 'an', '\"', 'itch', '\"', 'JOIN', 'yo', ',', 'U133', 'or', 'a', '\"', 'ogan', '\"', 'are', 'you', 'a', 'male', '?', 'JOIN', 'JOIN', 'show', 'will', 'let', \"'s\", 'talk', '.', 'PART', ':)', 'haha', 'brb', 'opps', 'JOIN', 'PART', 'sho', '*', '.', 'ACTION', 'keeps', 'U115', 's', 'place', 'nice', 'and', 'warm', '.', 'hey', 'any', 'guys', 'with', 'cams', 'wanna', 'play', '?', '.', 'ACTION', 'sits', 'on', 'U68', \"'s\", 'lap', '.', 'JOIN', 'JOIN', 'any', 'guyz', 'wanna', 'chat', 'hi', 'there', 'boo', ',', 'it', \"'s\", 'a', 'female', '.', 'hey', 'U126', 'PART', 'i', 'wonna', 'chat', 'PART', '24', 'f', 'nc', 'single', 'mom', 'where', 'did', 'everyone', 'gooo', '?', 'sure', 'U126', 'JOIN', 'what', 'did', 'you', 'but', 'on', 'e-bay', 'i', 'feel', 'like', 'im', 'in', 'the', 'wrong', 'room', 'yeee', 'haw', 'U30', 'im', 'considering', 'changing', 'my', 'nickname', 'to', '\"', 'ihavehotnips', '\"', 'JOIN', 'i', 'do', \"n't\", 'want', 'hot', 'pics', 'of', 'a', 'female', ',', 'I', 'can', 'look', 'in', 'a', 'mirror', '.', 'hi', 'U64', 'wb', 'U139', 'u', 'should', 'U44', ':)', 'PART', 'PART', 'JOIN', 'single', 'dad', 'here', 'JOIN', 'ty', 'U68', 'PART', 'JOIN', 'Hi', 'U139', 'PART', 'JOIN', 'hi', 'U138', 'HAHAHA', 'yw', 'U139', 'you', 'should', 'make', 'it']\n"
     ]
    }
   ],
   "source": [
    "print(all_posts[50:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
